# 相关工作
## 设计初衷
在当今数据驱动的时代，文件系统的性能和效率成为了衡量一个技术项目成功与否的关键指标之一。DeepSeek项目第五天的开源信息披露中，3FS并行文件系统的发布无疑为这一领域注入了一股强大的力量。3FS并行文件系统不仅仅是一个技术上的突破，更是对传统文件处理方式的一次深刻变革。

3FS并行文件系统的设计初衷是为了应对大规模数据存储和处理的需求。它采用了分布式架构，将数据分散存储在多个节点上，从而实现了高并发读写操作。这种设计不仅提高了系统的容错性，还极大地提升了数据访问的速度和效率。具体来说，3FS通过智能调度算法，能够根据各个节点的负载情况动态分配任务，确保整个系统的资源得到最优化利用。

此外，3FS并行文件系统还具备高度可扩展性。随着数据量的增长，用户可以通过简单地增加新的存储节点来扩展系统的容量，而无需对现有架构进行大规模改造。这种灵活性使得3FS能够在不同规模的应用场景中发挥出色的表现，无论是小型企业还是大型数据中心，都能从中受益。

值得一提的是，3FS并行文件系统在安全性方面也做了诸多改进。它支持多种加密协议，确保数据在传输和存储过程中的安全性。同时，系统内置了完善的权限管理机制，能够精确控制用户对文件的访问权限，有效防止未经授权的操作。[1]
## 与其他分布式系统的异同：
### HDFS
HDFS是一个易于扩展的分布式文件系统，运行在成百上千台低成本的机器上。它与现有的分布式文件系统有许多相似之处，都是用来存储数据的系统工具，而区别于HDFS具有高度容错能力，旨在部署在低成本机器上。HDFS主要用于对海量文件信息进行存储和管理，也就是解决大数据文件（如TB乃至PB级）的存储问题。
HDFS采用主从架构（Master/Slave架构）。
HDFS集群是由一个NameNode和多个 DataNode组成。
HDFS提供 SecondaryNameNode 辅助 NameNode。[2]
**NameNode（名称节点）**
NameNode是HDFS集群的主服务器，通常称为名称节点或者主节点。一旦NameNode关闭，就无法访问Hadoop集群。NameNode主要以元数据的形式进行管理和存储，用于维护文件系统名称并管理客户端对文件的访问；NameNode记录对文件系统名称空间或其属性的任何更改操作；HDFS负责整个数据集群的管理，并且在配置文件中可以设置备份数量，这些信息都由NameNode存储。

**DataNode（数据节点）**
DataNode是HDFS集群中的从服务器，通常称为数据节点。文件系统存储文件的方式是将文件切分成多个数据块，这些数据块实际上是存储在DataNode节点中的，因此DataNode机器需要配置大量磁盘空间。它与NameNode通过心跳监测机制保持不断的通信，DataNode在客户端或者NameNode的调度下，存储并检索数据块，对数据块进行创建、删除等操作，并且定期向NameNode发送所存储的数据块列表。
**SecondaryNameNode（辅助节点）**
SecondaryNameNode是HDFS集群中的辅助节点。定期从NameNode拷贝FsImage文件并合并Edits文件，将合并结果发送给NameNode。SecondaryNameNode和NameNode保存的FsImage和Edits文件相同，可以作为NameNode的冷备份，它的目的是帮助 NameNode合并编辑日志，减少NameNode启动时间。当NameNode宕机无法使用时，可以通过手动操作将SecondaryNameNode切换为NameNode。
**Block（数据块）**
每个磁盘都有默认的数据块大小，这是磁盘进行数据读/写的最小单位，HDFS同样也有块的概念，它是抽象的块，而非整个文件作为存储单元，在Hadoop3.x版本下，默认大小是128M，且备份3份，每个块尽可能地存储于不同的DataNode中。按块存储的好处主要是屏蔽了文件的大小，提供数据的容错性和可用性。
**Rack（机架）**
Rack是用来存放部署Hadoop集群服务器的机架，不同机架之间的节点通过交换机通信，HDFS通过机架感知策略，使NameNode能够确定每个DataNode所属的机架ID，使用副本存放策略，来改进数据的可靠性、可用性和网络带宽的利用率。
**Metadata（元数据）**
在 NameNode 内部是以元数据的形式，维护着两个文件，分别是FsImage 镜像文件和 EditLog 日志文件。其中，FsImage镜像文件用于存储整个文件系统命名空间的信息，EditLog日志文件用于持久化记录文件系统元数据发生的变化。
当 NameNode启动的时候，FsImage 镜像文件就会被加载到内存中，然后对内存里的数据执行记录的操作，以确保内存所保留的数据处于最新的状态，这样就加快了元数据的读取和更新操作，但是这些操作非常消耗NameNode资源。于是HDFS文件系统引入了 EditLog 日志文件，该文件以追加方式记录内存中元数据的每一次变化，如果NameNode宕机，可以通过合并FsImage文件和Edits文件的方式恢复内存中存储的元数据。[3]
### Ceph
#### Ceph简介
Ceph是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。
Ceph项目最早起源于Sage就读博士期间的工作（最早的成果于2004年发表），并随后贡献给开源社区。
在经过了数年的发展之后，目前已得到众多云计算厂商的支持并被广泛应用。
RedHat及OpenStack都可与Ceph整合以支持虚拟机镜像的后端存储。
#### Ceph特点
**高性能**
- 摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高。
- 考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。
- 能够支持上千个存储节点的规模，支持TB到PB级的数据。

**高可用性**
- 副本数可以灵活控制。
- 支持故障域分隔，数据强一致性。
- 多种故障场景自动进行修复自愈。
- 没有单点故障，自动管理。

**高可扩展性**
- 去中心化。
- 扩展灵活。
- 随着节点增加而线性增长。

**特性丰富**
- 支持三种存储接口：块存储、文件存储、对象存储。
- 支持自定义接口，支持多种语言驱动。
#### Ceph核心组件及概念介绍
1. Monitor
一个Ceph集群需要多个Monitor组成的小集群，它们通过Paxos同步数据，用来保存OSD的元数据。

2. OSD
OSD全称Object Storage Device，也就是负责响应客户端请求返回具体数据的进程。一个Ceph集群一般都有很多个OSD。

3. MDS
MDS全称Ceph Metadata Server，是CephFS服务依赖的元数据服务。

4. Object
Ceph最底层的存储单元是Object对象，每个Object包含元数据和原始数据。

5. PG
PG全称Placement Grouops，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据。

6. RADOS
RADOS全称Reliable Autonomic Distributed Object Store，是Ceph集群的精华，用户实现数据分配、Failover等集群操作。
7. Libradio
Librados是Rados提供库，因为RADOS是协议很难直接访问，因此上层的RBD、RGW和CephFS都是通过librados访问的，目前提供PHP、Ruby、Java、Python、C和C++支持。

8. CRUSH
CRUSH是Ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。

9. RBD
RBD全称RADOS block device，是Ceph对外提供的块设备服务。

10. RGW
RGW全称RADOS gateway，是Ceph对外提供的对象存储服务，接口与S3和Swift兼容。

11. CephFS
CephFS全称Ceph File System，是Ceph对外提供的文件系统服务。

12. Pool：存储池，是存储对象的逻辑分区，它规定了数据冗余的类型和对应的副本
分布策略；支持两种类型：副本（replicated）和 纠删码（Erasure Code）
 一个Pool里有很多PG；
 一个PG里包含一堆对象；一个对象只能属于一个PG；
 PG属于多个OSD，分布在不同的OSD上[4]
## 3FS应用领域：smallpond
1. Smallpond 是什么？
**(1) 定义**
Smallpond 是一个专为大规模 AI 训练设计的高性能、分布式数据加载和预处理框架。它由上海交通大学 IPADS 实验室开发，旨在解决 AI 训练中数据 I/O 瓶颈问题，特别是与 3FS 文件系统结合使用时，能够显著提升数据加载和预处理的效率。Smallpond 的设计理念是“小数据池”（small pond），通过将大规模数据集划分为多个小数据块（chunks），并利用分布式缓存和并行处理技术，实现高效的数据访问和预处理。
**(2) 核心功能**
- 分布式数据加载：Smallpond 支持从 3FS 等分布式文件系统高效加载数据，将数据分块并分发到多个计算节点。
- 并行数据预处理：Smallpond 可以在多个计算节点上并行执行数据预处理操作，如图像增强、文本编码、特征提取等。
- 分布式缓存：Smallpond 内置分布式缓存机制，可以将常用的数据块缓存在内存或 SSD 中，减少重复 I/O 操作。
- 数据流水线：Smallpond 支持构建数据流水线，将数据加载、预处理、缓存等操作组合成一个高效的工作流。
- 与 3FS 集成：Smallpond 针对 3FS 文件系统进行了优化，能够充分利用 3FS 的高性能和高可扩展性。
- 易用性：Smallpond 提供了简洁易用的 API，方便用户定义和管理数据加载和预处理流程。[5]
-  Smallpond + 3FS 实际应用案例
**DeepSeek-AI 大规模模型训练**
- 实现：DeepSeek-AI 使用 3FS 作为底层存储系统，Smallpond 作为数据加载和预处理框架。
- 流程：
Smallpond 从 3FS 读取训练数据，将数据分块并分发到多个计算节点。
在每个节点上，Smallpond 并行执行数据预处理操作，如文本分词、编码、填充等。
预处理后的数据被送入 AI 模型进行训练。
Smallpond 的分布式缓存机制缓存常用的数据块，减少重复 I/O 操作。
- 效果：
- 性能提升：Smallpond 与 3FS 的结合显著提升了数据加载和预处理的效率，缩短了模型训练周期。
- 扩展性优势：Smallpond 和 3FS 的高可扩展性使得 DeepSeek-AI 能够轻松扩展到数千个节点，支持更大规模的模型训练。
- 资源利用率提高：Smallpond 的并行处理和缓存机制提高了计算资源和存储资源的利用率。

参考文献
[1]https://blog.csdn.net/Xs_20240309/article/details/145936389
[2]https://blog.csdn.net/972301/article/details/145983604
[3]https://blog.csdn.net/HinsCoder/article/details/136440714
[4]https://blog.csdn.net/m0_58833554/article/details/134604853
[5]https://blog.csdn.net/972301/article/details/145983604